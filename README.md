# âœ‹ Gesture-Controlled Virtual Keyboard  

A real-time **gesture-based virtual keyboard** that allows users to perform search queries on **Google, YouTube, and Instagram** using only their handsâ€”no physical input devices required.  

This project uses **computer vision** and **hand tracking** to detect fingertip positions and recognize tap gestures, enabling text input through an on-screen keyboard. Once a query is typed, the system automatically launches a search in the browser.  

---

## ğŸš€ Features  
- **Real-Time Gesture Input** â€“ Detects fingertip taps using MediaPipe hand landmarks.  
- **Virtual Keyboard** â€“ Dynamic, color-coded keyboard fully rendered with OpenCV.  
- **Multi-Platform Support** â€“ Perform searches on Google, YouTube, and Instagram.  
- **Auto Search Execution** â€“ Automatically launches search after a short pause.  
- **Visual Feedback** â€“ Displays typed text, gesture detection, and active platform.  

---

## ğŸ› ï¸ Tech Stack  
- **Python** â€“ Core language for logic and flow.  
- **OpenCV** â€“ Real-time video capture, rendering, and image processing.  
- **MediaPipe** â€“ Hand tracking and landmark detection.  
- **NumPy** â€“ Numerical operations (e.g., distance calculations).  
- **webbrowser module** â€“ Launches search queries directly in the browser.  

---

## ğŸ¯ Real-World Use Cases  
- **Assistive Technology** â€“ Helps users with mobility impairments interact without keyboards or mice.  
- **Public Interfaces** â€“ Safer touchless input for kiosks and smart displays.  
- **Creative Applications** â€“ Interactive installations, education tools, and futuristic UX designs.  

---

## ğŸ”® Future Improvements  
- Gesture-based scrolling & navigation for search results.  
- Custom gesture training with ML models.  
- Multilingual keyboard support.  
- Chrome extension integration for direct browser control.  

---

## ğŸ™ Acknowledgments  
Special thanks to **Muhammad Usama** for mentorship and guidance throughout this project.  

---
